{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home2/pygenest/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home2/pygenest/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article, fulltext\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "tqdm.pandas()\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import wordninja\n",
    "\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 157/19783 [00:02<04:47, 68.19it/s]/tmp/ipykernel_3426186/2087471578.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  train_data['html_soup'] = train_data['html'].progress_apply(lambda html: BeautifulSoup(html, 'lxml'))\n",
      "  1%|          | 226/19783 [00:04<07:59, 40.76it/s]/home2/pygenest/miniconda3/envs/hackathon_passau/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 61%|██████    | 12075/19783 [03:31<01:31, 84.27it/s] /tmp/ipykernel_3426186/2087471578.py:4: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  train_data['html_soup'] = train_data['html'].progress_apply(lambda html: BeautifulSoup(html, 'lxml'))\n",
      "100%|██████████| 19783/19783 [05:43<00:00, 57.64it/s] \n"
     ]
    }
   ],
   "source": [
    "train_labels = pd.read_json('Hackathon_data/train/D1_train-truth.jsonl', orient='records', lines=True).set_index('uid', drop=True).sort_index()['label']\n",
    "train_data = pd.read_json('Hackathon_data/train/D1_train.jsonl', orient='records', lines=True).set_index('uid', drop=True).sort_index()\n",
    "train_data['label'] = train_labels\n",
    "\n",
    "train_data['html_soup'] = train_data['html'].progress_apply(lambda html: BeautifulSoup(html, 'lxml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 777/2937 [00:08<00:16, 132.13it/s]/home2/pygenest/miniconda3/envs/hackathon_passau/lib/python3.10/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      " 28%|██▊       | 813/2937 [00:08<00:14, 146.27it/s]/tmp/ipykernel_3426186/3243447638.py:4: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  val_data['html_soup'] = val_data['html'].progress_apply(lambda html: BeautifulSoup(html, 'lxml'))\n",
      "100%|██████████| 2937/2937 [00:48<00:00, 61.03it/s] \n"
     ]
    }
   ],
   "source": [
    "val_labels = pd.read_json('Hackathon_data/validation/D1_validation-truth.jsonl', orient='records', lines=True).set_index('uid', drop=True).sort_index()['label']\n",
    "val_data = pd.read_json('Hackathon_data/validation/D1_validation.jsonl', orient='records', lines=True).set_index('uid', drop=True).sort_index()\n",
    "\n",
    "val_data['html_soup'] = val_data['html'].progress_apply(lambda html: BeautifulSoup(html, 'lxml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_translation = {\n",
    "    'en': 'english',\n",
    "    'de': 'german',\n",
    "    'hr': 'hungarian',\n",
    "    'nl': 'dutch',\n",
    "    'ru': 'russian',\n",
    "    'zh-cn': 'chinese',\n",
    "    'pt': 'portuguese',\n",
    "    'fr': 'french',\n",
    "    'id': 'indonesia',\n",
    "    'es': 'spanish',\n",
    "    'tr': 'turkish',\n",
    "    'it': 'italian'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {}\n",
    "for key, lang in language_translation.items():\n",
    "    try:\n",
    "        stop_words[key] = stopwords.words(lang)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmers = {}\n",
    "for key, lang in language_translation.items():\n",
    "    try:\n",
    "        stemmers[key] = SnowballStemmer(language=lang)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text: str):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def preprocess_text(row) -> str:\n",
    "    lang = row['lang']\n",
    "    text = row['text']\n",
    "    text = text.lower()\n",
    "    tokens = [w for w in word_tokenize(text)]\n",
    "\n",
    "    # Handle stopwords\n",
    "    if lang in stop_words:\n",
    "        tokens = [w for w in tokens if w not in stop_words[lang]]\n",
    "    if lang in stemmers:\n",
    "        stemmer = stemmers[lang]\n",
    "        tokens = [stemmer.stem(w) for w in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=15000)\n",
    "\n",
    "def preprocess_features(dataset: pd.DataFrame, train=True):\n",
    "    #dataset['text'] = dataset['html_soup'].progress_apply(lambda soup: soup.text)\n",
    "    #dataset['lang'] = dataset['text'].progress_apply(detect_language)\n",
    "    #dataset['processed_text'] = dataset.progress_apply(preprocess_text, axis='columns')\n",
    "    if train:\n",
    "        tokens = vectorizer.fit_transform(dataset['processed_text'])\n",
    "    else:\n",
    "        tokens = vectorizer.transform(dataset['processed_text'])\n",
    "    return tokens\n",
    "\n",
    "train_features = preprocess_features(train_data, train=True)\n",
    "val_features = preprocess_features(val_data, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "model = SGDClassifier(loss='modified_huber')\n",
    "#model = SVC(kernel='rbf')\n",
    "#model = GradientBoostingClassifier()\n",
    "model.fit(train_features, train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = model.predict(train_features)\n",
    "val_predictions = model.predict(val_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Adult       1.00      0.99      1.00      4301\n",
      "      Benign       0.93      0.91      0.92      4789\n",
      "   Malicious       0.96      0.97      0.96     10693\n",
      "\n",
      "    accuracy                           0.96     19783\n",
      "   macro avg       0.96      0.96      0.96     19783\n",
      "weighted avg       0.96      0.96      0.96     19783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_labels, train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Adult       0.91      0.85      0.88        75\n",
      "      Benign       0.72      0.70      0.71       805\n",
      "   Malicious       0.88      0.89      0.89      2057\n",
      "\n",
      "    accuracy                           0.84      2937\n",
      "   macro avg       0.84      0.81      0.83      2937\n",
      "weighted avg       0.84      0.84      0.84      2937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain_and_path(url):\n",
    "    # Supprimer le protocole s'il est présent\n",
    "    url = url.replace('www', '')\n",
    "\n",
    "    # Extraire le nom de domaine et le chemin\n",
    "    parts = url.split('/')\n",
    "    domain = parts[0].replace('.', ' ')\n",
    "    path = ' '.join(parts[1:])\n",
    "\n",
    "    # Concaténer le nom de domaine et le chemin\n",
    "    result = f\"{domain} {path}\"\n",
    "\n",
    "    return result.strip()\n",
    "\n",
    "def separate_words(input_str):\n",
    "    # Utiliser wordninja pour segmenter les mots\n",
    "    words = wordninja.split(input_str)\n",
    "\n",
    "    # Joindre les mots séparés avec des espaces\n",
    "    result = ' '.join(words)\n",
    "\n",
    "    return result\n",
    "\n",
    "def top_words(input_str, n=1000):\n",
    "    input_str = [word for word in word_tokenize(input_str) if len(word) > 1]\n",
    "    input_str = ' '.join(input_str)\n",
    "    # Convertir le texte en minuscules et diviser en mots\n",
    "    words = re.findall(r'\\b\\w+\\b', input_str.lower())\n",
    "\n",
    "    # Compter les occurrences des mots\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Afficher les n mots les plus courants\n",
    "    top_n_words = word_counts.most_common(1000)\n",
    "    \n",
    "    L = []\n",
    "    for word, count in top_n_words:\n",
    "        L.append(word)\n",
    "    return L\n",
    "    \n",
    "def tokenize_url(url, corpus):\n",
    "    v = np.zeros(len(corpus) + 3, dtype=int)\n",
    "    for idx, w in enumerate(corpus):\n",
    "        n = url.count(w)\n",
    "        if n > 0:\n",
    "            v[idx] = n\n",
    "    v[-1] = 1 if url.count('/') > 2 else 0\n",
    "    v[-2] = 0 if sum(c.isdigit() for c in url) == 0 else 1\n",
    "    v[-3] = 1 if url.count('-') > 0 else 0\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_url_dataset(dataset: pd.DataFrame, train=True, corpus=None):\n",
    "    dataset['url_words'] = dataset['url'].apply(lambda url: separate_words(extract_domain_and_path(url)))\n",
    "    if train:\n",
    "        grouped_df = dataset.groupby('label')['url_words'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "        top_words_result_A = top_words(grouped_df['url_words'][0])\n",
    "        top_words_result_B = top_words(grouped_df['url_words'][1])\n",
    "        top_words_result_M = top_words(grouped_df['url_words'][2])\n",
    "        corpus = list(set(top_words_result_A + top_words_result_B + top_words_result_M))\n",
    "\n",
    "    dataset['url_vector'] = dataset['url'].apply(lambda url: tokenize_url(url, corpus))\n",
    "    return dataset['url_vector'].tolist(), corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features4, corpus = preprocess_url_dataset(train_data, train=True)\n",
    "val_features4, corpus = preprocess_url_dataset(val_data, train=False, corpus=corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = SGDClassifier(loss='modified_huber')\n",
    "model4.fit(train_features4, train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions4 = model4.predict(train_features4)\n",
    "val_predictions4 = model4.predict(val_features4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Adult       1.00      1.00      1.00      4301\n",
      "      Benign       0.98      0.99      0.99      4789\n",
      "   Malicious       1.00      0.99      1.00     10693\n",
      "\n",
      "    accuracy                           0.99     19783\n",
      "   macro avg       0.99      0.99      0.99     19783\n",
      "weighted avg       0.99      0.99      0.99     19783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_labels, train_predictions4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Adult       0.67      0.87      0.76        75\n",
      "      Benign       0.92      0.92      0.92       805\n",
      "   Malicious       0.98      0.97      0.97      2057\n",
      "\n",
      "    accuracy                           0.95      2937\n",
      "   macro avg       0.86      0.92      0.88      2937\n",
      "weighted avg       0.96      0.95      0.95      2937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, val_predictions4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_proba = model.predict_proba(train_features)\n",
    "val_pred_proba = model.predict_proba(val_features)\n",
    "train_pred_proba4 = model4.predict_proba(train_features4)\n",
    "val_pred_proba4 = model4.predict_proba(val_features4)\n",
    "\n",
    "train_features_final = np.concatenate([train_pred_proba, train_pred_proba4], axis=1)\n",
    "val_features_final = np.concatenate([val_pred_proba, val_pred_proba4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = SGDClassifier()\n",
    "model_final.fit(train_features_final, train_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_final = model_final.predict(train_features_final)\n",
    "val_predictions_final = model_final.predict(val_features_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Adult       1.00      1.00      1.00      4301\n",
      "      Benign       1.00      1.00      1.00      4789\n",
      "   Malicious       1.00      1.00      1.00     10693\n",
      "\n",
      "    accuracy                           1.00     19783\n",
      "   macro avg       1.00      1.00      1.00     19783\n",
      "weighted avg       1.00      1.00      1.00     19783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(train_labels, train_predictions_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Adult       0.88      0.92      0.90        75\n",
      "      Benign       0.95      0.94      0.94       805\n",
      "   Malicious       0.98      0.98      0.98      2057\n",
      "\n",
      "    accuracy                           0.97      2937\n",
      "   macro avg       0.94      0.95      0.94      2937\n",
      "weighted avg       0.97      0.97      0.97      2937\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(val_labels, val_predictions_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon_passau",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
